{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語処理100本ノック 000 ~ 009\n",
    "## モチベーション\n",
    "* Pythonに慣れる\n",
    "* NLPに慣れる\n",
    "* Jupyterに慣れる\n",
    "\n",
    "## 方針\n",
    "* 現場で使いそうなライブラリ、関数はガンガン使ってよし\n",
    "* 短い記述が好き(個人的好み\n",
    "\n",
    "## 感想\n",
    "* 例題がおもしろくて、やってて楽しい、いい問題集だ\n",
    "* Pythonも色々とやり方があり、実装者の癖がかなり出る(クソコーダですみません"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (0)文字列\"stressed\"の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ\n",
    "* [ start:end:step ]によるスライス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desserts'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"stressed\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．\n",
    "* スライスの応用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'タクシー'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"パタトクカシーー\"[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．\n",
    "* リスト内包\n",
    "* join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'パタトクカシーー'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"パトカー\"\n",
    "s2 = \"タクシー\"\n",
    "''.join( c1 + c2 for c1, c2 in zip(s1, s2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)\"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．\n",
    "\n",
    "* nltk.tokenize.word_tokenize\n",
    "* だけでは足りなかったので、正規表現も\n",
    "* pat.match は pat.search と異なり、前方からマッチしなければ弾いてしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import re\n",
    "pat = re.compile(r\"\\w+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "[ len(w) for w in tokenize.word_tokenize(s) if pat.match(w) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4)\"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭に2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．\n",
    "\n",
    "* RegexpTokenizer\n",
    "* enumerate\n",
    "* 三項演算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "idx = {1, 5, 6, 7, 8, 9, 15, 16, 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'He',\n",
       " 'Lied',\n",
       " 'Because',\n",
       " 'Boron',\n",
       " 'Could',\n",
       " 'Not',\n",
       " 'Oxidize',\n",
       " 'Fluorine',\n",
       " 'New',\n",
       " 'Nations',\n",
       " 'Might',\n",
       " 'Also',\n",
       " 'Sign',\n",
       " 'Peace',\n",
       " 'Security',\n",
       " 'Clause',\n",
       " 'Arthur',\n",
       " 'King',\n",
       " 'Can']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [w for w in tokenizer.tokenize(s) ]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Hi',\n",
       " 1: 'H',\n",
       " 2: 'Li',\n",
       " 3: 'Be',\n",
       " 4: 'Bo',\n",
       " 5: 'C',\n",
       " 6: 'N',\n",
       " 7: 'O',\n",
       " 8: 'F',\n",
       " 9: 'N',\n",
       " 10: 'Na',\n",
       " 11: 'Mi',\n",
       " 12: 'Al',\n",
       " 13: 'Si',\n",
       " 14: 'Pe',\n",
       " 15: 'S',\n",
       " 16: 'C',\n",
       " 17: 'Ar',\n",
       " 18: 'Ki',\n",
       " 19: 'C'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ i: ( w[0] if i in idx else w[0:2] )\n",
    "    for i, w in enumerate(a) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，\"I am an NLPer\"という文から単語bi-gram，文字bi-gramを得よ．\n",
    "\n",
    "* range\n",
    "* len\n",
    "* each_cons を連想してググったら出てきた itertools.tee 微妙だったので見送り\n",
    "* join(x) のxに文字列が入っても問題ないっぽい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(sec,n=2):\n",
    "    return [ '-'.join(sec[i:i+n]) for i in range(len(sec)-n+1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I-am', 'am-an', 'an-NLPer'],\n",
       " ['I- ',\n",
       "  ' -a',\n",
       "  'a-m',\n",
       "  'm- ',\n",
       "  ' -a',\n",
       "  'a-n',\n",
       "  'n- ',\n",
       "  ' -N',\n",
       "  'N-L',\n",
       "  'L-P',\n",
       "  'P-e',\n",
       "  'e-r']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    ngram(tokenize.word_tokenize(\"I am an NLPer\"),2),\n",
    "    ngram(\"I am an NLPer\",2),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) \"paraparaparadise\"と\"paragraph\"に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = { g for g in ngram(\"paraparaparadise\",2) }\n",
    "Y = { g for g in ngram(\"paragraph\",2) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r-a', 'a-r', 'p-a', 'a-d', 'a-p', 'd-i', 'i-s', 's-e'}\n",
      "{'r-a', 'a-r', 'p-h', 'p-a', 'a-g', 'a-p', 'g-r'}\n",
      "{'a-r', 'p-a', 'a-g', 'a-d', 'p-h', 'g-r', 's-e', 'r-a', 'a-p', 'd-i', 'i-s'}\n",
      "{'a-r', 'p-a', 'r-a', 'a-p'}\n",
      "{'a-d', 'd-i', 'i-s', 's-e'}\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)\n",
    "print(X|Y)\n",
    "print(X&Y)\n",
    "print(X-Y)\n",
    "print('s-e' in X)\n",
    "print('s-e' in Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) 引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=\"気温\", z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,y,z):\n",
    "    return \"%s時の%sは%s\" % (x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12時の気温は22.4'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=12\n",
    "y=\"気温\"\n",
    "z=22.4\n",
    "f(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) 与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "* 英小文字ならば(219 - 文字コード)の文字に置換\n",
    "* その他の文字はそのまま出力\n",
    "\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．\n",
    "\n",
    "* これは、アトバシュ暗号(同一関数で暗号化と復号が可能)なんだって、へー\n",
    "* islower() と regpat.match はどっちが速いんだろう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cipher(string):\n",
    "    return ''.join(\n",
    "        chr( 219 - ord(c) ) if c.islower() else c\n",
    "        for c in string\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I zn zm NLPvi'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cipher(\"I am an NLPer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば\"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"）を与え，その実行結果を確認せよ．\n",
    "* random.shuffle は破壊的\n",
    "* 流石にリスト内包表記は書けなかった\n",
    "* しかしappendはリスト内包表記よりも若干遅い、諦めるしかないのか\n",
    "* readable にはなった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def randomer(string):\n",
    "    res = []\n",
    "    for w in string.split(\" \"):\n",
    "        if len(w) <= 4:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            li = list(w[1:-1])\n",
    "            random.shuffle(li)\n",
    "            res.append( w[0] + ''.join(li) + w[-1] )\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I cdlonu't bevilee that I cluod aactully urnatendsd what I was rdianeg : the poenmaehnl power of the hamun mind .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
    "randomer(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
